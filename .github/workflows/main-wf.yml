name: GitHub Actions Demo
run-name: K8s-Deploy

on:
  workflow_dispatch:
    inputs:
      user_name:
        description: "User's Name"
        required: true
      awsAccessKey:
        description: "AWS Access Key"
        required: true
      awsSecretAccessKey:
        description: "AWS Secret Access Key"
        required: true
      aws_region:
        description: "AWS region to deploy resources"
        required: false
        default: "us-east-1"
      ec2_count:
        description: "Number of worker EC2 instances to create"
        required: false
        default: "3"
      ami_id:
        description: "AMI ID for the EC2 instances"
        required: false
        default: "ami-04b4f1a9cf54c11d0"
      instance_type:
        description: "Instance type for the EC2 instances"
        required: false
        default: "t3.micro"

jobs:
  Provision:
    runs-on: ubuntu-latest
    steps:
      - name: ${{ github.event.inputs.user_name }}
        uses: actions/checkout@v3

      # Checkout the repository to access scripts, Terraform, and Ansible files
      - name: Checkout code
        uses: actions/checkout@v3

      # Generate RSA SSH Key
      - name: Generate RSA SSH Key
        run: |
          ssh-keygen -t rsa -b 4096 -m PEM -f ssh-key.pem -N ""
          mkdir ~/.ssh
          cat ssh-key.pem > ~/.ssh/id_rsa
          cat ssh-key.pem.pub > ssh-key_pub.pem
        id: ssh

      # Make scripts executable
      - name: Make Scripts Executable
        run: chmod +x ./scripts/*.sh

      # Check and Terminate EC2 Instances (Master or Worker-n) if Exists
      - name: Check and Terminate EC2 Instances if Exists
        run: |
          # Set AWS credentials explicitly
          export AWS_ACCESS_KEY_ID=${{ github.event.inputs.awsAccessKey }}
          export AWS_SECRET_ACCESS_KEY=${{ github.event.inputs.awsSecretAccessKey }}
          export AWS_REGION=${{ github.event.inputs.aws_region }}

          # Get instance IDs for 'Master' and 'Worker-1' to 'Worker-10' using AWS CLI
          INSTANCE_IDS=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=Master,Worker-1,Worker-2,Worker-3,Worker-4,Worker-5,Worker-6,Worker-7,Worker-8,Worker-9,Worker-10" \
            --query "Reservations[].Instances[].InstanceId" \
            --output text)

          # Check if any matching instances are found
          if [ -n "$INSTANCE_IDS" ]; then
            echo "Terminating EC2 instances: $INSTANCE_IDS"
            aws ec2 terminate-instances --instance-ids $INSTANCE_IDS
            aws ec2 wait instance-terminated --instance-ids $INSTANCE_IDS
          else
            echo "No matching EC2 instances (Master or Worker-n) found."
          fi

      # Check and Delete S3 Bucket if Exists
      - name: Check and Delete S3 Bucket if Exists
        run: |
          export AWS_ACCESS_KEY_ID=${{ github.event.inputs.awsAccessKey }}
          export AWS_SECRET_ACCESS_KEY=${{ github.event.inputs.awsSecretAccessKey }}
          export AWS_REGION=${{ github.event.inputs.aws_region }}

          # Check if the S3 bucket exists
          echo "Checking if the S3 bucket k8s-infra-ssh-key exists..."

          # Attempt to access the bucket. If it fails (bucket doesn't exist), handle it
          aws s3api head-bucket --bucket k8s-infra-ssh-key 2>&1 && {
            echo "No issues with bucket existence. Proceeding to delete if exists."

            # If no error, delete the bucket
            aws s3 rb s3://k8s-infra-ssh-key --force
          }

          echo "No existing S3 bucket found."



      # Check and Delete Security Groups if Exists
      - name: Check and Delete Security Groups if Exists
        run: |
          export AWS_ACCESS_KEY_ID=${{ github.event.inputs.awsAccessKey }}
          export AWS_SECRET_ACCESS_KEY=${{ github.event.inputs.awsSecretAccessKey }}
          export AWS_REGION=${{ github.event.inputs.aws_region }}

          MASTER_SG=$(aws ec2 describe-security-groups --filters Name=group-name,Values=sg_master --query "SecurityGroups[].GroupId" --output text)
          WORKER_SG=$(aws ec2 describe-security-groups --filters Name=group-name,Values=sg_worker --query "SecurityGroups[].GroupId" --output text)

          if [ -n "$MASTER_SG" ]; then
            echo "Deleting existing master security group $MASTER_SG"
            aws ec2 delete-security-group --group-id $MASTER_SG
          else
            echo "No existing master security group found."
          fi

          if [ -n "$WORKER_SG" ]; then
            echo "Deleting existing worker security group $WORKER_SG"
            aws ec2 delete-security-group --group-id $WORKER_SG
          else
            echo "No existing worker security group found."
          fi

      # Check and Delete EC2 Key Pair if Exists
      - name: Check and Delete EC2 Key Pair if Exists
        run: |
          export AWS_ACCESS_KEY_ID=${{ github.event.inputs.awsAccessKey }}
          export AWS_SECRET_ACCESS_KEY=${{ github.event.inputs.awsSecretAccessKey }}
          export AWS_REGION=${{ github.event.inputs.aws_region }}

          # Check if the EC2 key pair exists
          echo "Checking if EC2 key pair 'master-key' exists..."

          # Attempt to describe the key pair. If it succeeds, delete the key pair
          aws ec2 describe-key-pairs --key-name master-key --query "KeyPairs[0].KeyName" --output text 2>&1 && {
            echo "Key pair 'master-key' exists. Deleting it..."
            aws ec2 delete-key-pair --key-name master-key
          }

          # If the key pair does not exist, handle it
          echo "Key pair 'master-key' does not exist."


      # Install Terraform
      - name: Install Terraform
        run: ./scripts/install-terraform.sh

      # Initialize and Apply Terraform
      - name: Terraform Init and Apply
        env:
          AWS_ACCESS_KEY_ID: ${{ github.event.inputs.awsAccessKey }}
          AWS_SECRET_ACCESS_KEY: ${{ github.event.inputs.awsSecretAccessKey }}
        run: |
          cd terraform-files
          terraform init
          terraform apply -auto-approve \
            -var="aws_region=${{ github.event.inputs.aws_region }}" \
            -var="ec2_count=${{ github.event.inputs.ec2_count }}" \
            -var="ami_id=${{ github.event.inputs.ami_id }}" \
            -var="instance_type=${{ github.event.inputs.instance_type }}" \
            -var="ssh_public_key=$(cat ../ssh-key_pub.pem)" \
            -var="s3_object_source=../ssh-key.pem"

          # Capture the output IPs
          terraform output -json public_ips > ../ansible-files/ips.json
          cat ../ansible-files/ips.json

      # Prepare Ansible Inventory File
      - name: Generate Ansible Inventory
        run: |
          jq -r '
            to_entries[] |
            if .key == 0 then "[master]\n" + .value + "\n\n[worker]"
            else .value
            end
          ' ansible-files/ips.json > ansible-files/inventory.ini

          # Append Ansible vars to the inventory
          echo "[all:vars]" >> ansible-files/inventory.ini
          echo "ansible_user=ubuntu" >> ansible-files/inventory.ini
          echo "ansible_ssh_private_key_file=~/.ssh/id_rsa" >> ansible-files/inventory.ini

          cat ansible-files/inventory.ini

      # Disable Strict Host Key Checking To Stop It From Prompting During SSH
      - name: Disable Strict Host Key Checking
        run: echo -e "Host *\n\tStrictHostKeyChecking no\n" >> ~/.ssh/config

      # Add all IP addresses to known_hosts
      - name: Add IPs to known_hosts
        run: |
          cd ansible-files
          IP_LIST=$(jq -r '.[]' ips.json)
          for ip in $IP_LIST; do
            echo "Adding $ip to known_hosts"
            ssh-keyscan -H $ip >> ~/.ssh/known_hosts || true
          done
          cat ~/.ssh/known_hosts

      # Ping Instances and Retry if Failed
      - name: Ping Instances (Retry if Failed)
        id: ping
        run: |
          cd ansible-files
          sudo chmod 600 ~/.ssh/id_rsa
          retries=3
          count=0
          success=false
          while [ $count -lt $retries ]; do
            echo "Attempt $((count + 1)) to ping the instances..."
            
            # Run ansible ping module on all instances
            ansible all -i inventory.ini -m ping || {
              echo "Ping failed, retrying in 10 seconds..."
              sleep 10
            }
      
            # Check if ansible ping was successful
            if [ $? -eq 0 ]; then
              success=true
              break
            fi
      
            count=$((count + 1))
          done
      
          if [ "$success" = false ]; then
            echo "Ping failed after 3 attempts. Aborting deployment."
            echo "success=false" >> $GITHUB_ENV
            exit 1
          fi
      
          echo "success=true" >> $GITHUB_ENV
    

      # Run Ansible Playbook
      - name: Run Ansible Playbook
        if: env.success == 'true'
        run: |
          cd ansible-files
          sudo chmod 600 ~/.ssh/id_rsa
          ansible-playbook -i inventory.ini playbook.yml -vv
